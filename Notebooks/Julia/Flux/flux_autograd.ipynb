{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980252af-1125-4ba1-8b08-9f18841b1d8d",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Neural Networks and Deep Learning</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Flux Introduction</h2>\n",
    "\n",
    "For the non-initiated, Flux.jl is currently (september/2021) the most starred package in the\n",
    "Julia ecossystem, and it's the go-to package in terms of Deep Neural Networks for Julia.\n",
    "\n",
    "\n",
    "Autograd: Automatic Differentiation\n",
    "===================================\n",
    "\n",
    "Automatic differentiation (the topic of this notebook) is \n",
    "Flux's core feature. It take a Julia function `f` and a set of arguments, returning the\n",
    "gradient with respect to each argument.\n",
    "\n",
    "\n",
    "*Ref: This tutorial is based on\n",
    "[this example tutorial from Flux's github page](https://github.com/FluxML/model-zoo/blob/master/tutorials/60-minute-blitz/60-minute-blitz.jl).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4600304-8c9c-460c-82d3-fc277938f0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's import Flux as a package. This comes with the function `gradient` from Zygote.jl\n",
    "using Flux\n",
    "\n",
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "# Returns the gradient at 0.0.\n",
    "gradient(f,0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8fc8e-764c-43d0-a9a5-47c45cc04772",
   "metadata": {},
   "source": [
    "The `gradient` function uses automatic differentiation to calculate\n",
    "the derivative of polynomials.\n",
    "\n",
    "This does no work for any arbitrary\n",
    "function. Try for example, `f(x) = exp(x)` and you'll get an error.\n",
    "\n",
    "Below, we write another example for a function of three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6157f0b9-049a-4eba-a7fb-f3408a7720fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, 1.0, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x,y,z) = y^3 + x^3 + x*y + z\n",
    "\n",
    "gradient(h,1,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2d799-acc8-4a45-a9e5-682b232cb6d0",
   "metadata": {},
   "source": [
    "Now it's where things get interesting. We can take gradients\n",
    "of arrays.\n",
    "\n",
    "Take for example\n",
    "$Ax + b$, where $A$ is a 2 by 2 matrix, $x$ and $b$ are two dimensional vectors.\n",
    "This function actually returns another vector. So there is no gradient,\n",
    "but a jacobian. For this situation, we use the `jacobian` function from Zygote,\n",
    "which is not shipped with Flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc514ca-93a6-4395-8a4c-82f3f9fc5b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0 0 0 0; 0 0 0 0], [1 2; 3 3], [1 0; 0 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Zygote: jacobian\n",
    "\n",
    "f(A,x,b) = A*x .+ b\n",
    "\n",
    "A = [1 2\n",
    "     3 3]\n",
    "b = [1,1]\n",
    "x = [0,0]\n",
    "\n",
    "jacobian(f,A,x,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a29e3-ad73-40c7-8db2-9d16c9d62f7c",
   "metadata": {},
   "source": [
    "The reason that `jacobian` is not shipped on Flux is that Neural Networks\n",
    "usually only require the gradient in order to perform backward propagation. Hence,\n",
    "instead of $Ax + b$, we have functions such as $\\sum^n_{i=1} (Ax)_i + b_i$, which does\n",
    "have a gradient. Look the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a5e1ba1-dee1-4424-b1d5-34a05009b46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0 0; 0 0], [4, 5], Fill(1, 2))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(A,x,b) = sum(A*x .+ b)\n",
    "\n",
    "A = [1 2\n",
    "     3 3]\n",
    "b = [1,1]\n",
    "x = [0,0]\n",
    "\n",
    "gradient(f,A,x,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b91d2-ab9c-4d21-ae9e-59e40f138ffc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><p>\n",
    "<strong>Obs: <\\strong>note the `Fill(1,2)` as the last element of the output of the `gradient` function.\n",
    "This is just a way to represent a vector of dimension 2 where all elements are equal to 1.\n",
    "if you want to underestand more about it, copy the code below and run it in a cell to see the output.\n",
    "\n",
    "```julia\n",
    "using FillArrays\n",
    "@show collect(Fill(1,2))\n",
    "@show collect(Fill(3.5,2,2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003463a3-fb95-472b-ba5c-bb06a568938d",
   "metadata": {},
   "source": [
    "It's even more impressive. It can take gradient of functions defined programmatically! \n",
    "Take a look at the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d83d215-d748-456f-8da0-ce2cc0ff9afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(gradient(mycrazyfunction, 0))[1] == exp(0) = true\n",
      "(gradient(mycrazyfunction, 1))[1] == cos(1) = true\n",
      "(gradient(mycrazyfunction, -10))[1] == 2 * -10 = true\n"
     ]
    }
   ],
   "source": [
    "function mycrazyfunction(x)\n",
    "    if x ≥ 1\n",
    "        return sin(x)\n",
    "    elseif -1 < x < 1\n",
    "        return exp(x)\n",
    "    else\n",
    "        return x^2\n",
    "    end\n",
    "end\n",
    "\n",
    "@show gradient(mycrazyfunction,0)[1] == exp(0)\n",
    "@show gradient(mycrazyfunction,1)[1] == cos(1)\n",
    "@show gradient(mycrazyfunction,-10)[1] == 2*-10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160691f-9961-493a-9a54-eb4a9dea5812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc930f89-9eef-492b-9798-c741f08ddda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " -3.2740154025668464\n",
       "  1.7271538165976312\n",
       "  1.0671549391866826\n",
       "  1.1165473529223182\n",
       "  0.94526273534884"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myloss(W, b, x) = sum(W * x .+ b)\n",
    "\n",
    "W = randn(3, 5)\n",
    "b = zeros(3)\n",
    "x = rand(5)\n",
    "\n",
    "gradient(myloss, W, b, x)[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7615a053-1819-4f79-8242-36d87ba18e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8441b24-9547-4ba7-8e80-5f327b1b34a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5885336103564676 0.08707929087479926 … 0.2158671035677726 0.05615764632970266; 0.5885336103564676 0.08707929087479926 … 0.2158671035677726 0.05615764632970266; 0.5885336103564676 0.08707929087479926 … 0.2158671035677726 0.05615764632970266], Fill(1.0, 3), [-0.2824871946941474, 1.215211566576241, 2.385026797366619, 2.2394062953720333, -1.6592430716463225])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic Differentiation\n",
    "# -------------------------\n",
    "\n",
    "# You probably learned to take derivatives in school. We start with a simple\n",
    "# mathematical function like\n",
    "\n",
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "f(5)\n",
    "\n",
    "# In simple cases it's pretty easy to work out the gradient by hand – here it's\n",
    "# `6x+2`. But it's much easier to make Flux do the work for us!\n",
    "\n",
    "df(x) = gradient(f, x)[1]\n",
    "\n",
    "df(5)\n",
    "\n",
    "# You can try this with a few different inputs to make sure it's really the same\n",
    "# as `6x+2`. We can even do this multiple times (but the second derivative is a\n",
    "# fairly boring `6`).\n",
    "\n",
    "ddf(x) = gradient(df, x)[1]\n",
    "\n",
    "ddf(5)\n",
    "\n",
    "# Flux's AD can handle any Julia code you throw at it, including loops,\n",
    "# recursion and custom layers, so long as the mathematical functions you call\n",
    "# are differentiable. For example, we can differentiate a Taylor approximation\n",
    "# to the `sin` function.\n",
    "\n",
    "mysin(x) = sum((-1)^k*x^(1+2k)/factorial(1+2k) for k in 0:5)\n",
    "\n",
    "x = 0.5\n",
    "\n",
    "mysin(x), gradient(mysin, x)\n",
    "#-\n",
    "sin(x), cos(x)\n",
    "\n",
    "# You can see that the derivative we calculated is very close to `cos(x)`, as we\n",
    "# expect.\n",
    "\n",
    "# This gets more interesting when we consider functions that take *arrays* as\n",
    "# inputs, rather than just a single number. For example, here's a function that\n",
    "# takes a matrix and two vectors (the definition itself is arbitrary)\n",
    "\n",
    "myloss(W, b, x) = sum(W * x .+ b)\n",
    "\n",
    "W = randn(3, 5)\n",
    "b = zeros(3)\n",
    "x = rand(5)\n",
    "\n",
    "gradient(myloss, W, b, x)\n",
    "\n",
    "# Now we get gradients for each of the inputs `W`, `b` and `x`, which will come\n",
    "# in handy when we want to train models.\n",
    "\n",
    "# Because ML models can contain hundreds of parameters, Flux provides a slightly\n",
    "# different way of writing `gradient`. We instead mark arrays with `param` to\n",
    "# indicate that we want their derivatives. `W` and `b` represent the weight and\n",
    "# bias respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b3aa5-bdc9-45c2-a6f4-ae4343e5094b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaFlux 1.6.1",
   "language": "julia",
   "name": "juliaflux-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
